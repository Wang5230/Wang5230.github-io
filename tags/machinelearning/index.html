<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>tag: machinelearning | Kopaba</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Speak softly and carry a big stick; you will go far">
<meta property="og:type" content="website">
<meta property="og:title" content="Kopaba">
<meta property="og:url" content="https://www.wang5230.github.io/tags/machinelearning/index.html">
<meta property="og:site_name" content="Kopaba">
<meta property="og:description" content="Speak softly and carry a big stick; you will go far">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kopaba">
<meta name="twitter:description" content="Speak softly and carry a big stick; you will go far">
  
    <link rel="alternate" href="/atom.xml" title="Kopaba" type="application/atom+xml">
  
  
  
  <link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/fancybox/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/css/main.css">
  <script src="/lib/jquery/dist/jquery.min.js"></script>
  
  
  
  
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head></html>
<body>
  <div id="wrapper">
    <header id="header" class="clearfix">
	<a id="logo" href="/" title="Kopaba">Kopaba</a>
	
		<p class="description">Imagination is the essence of discovery</p>
	
	<nav id="nav-menu" class="clearfix">
		<form id="search" method="post" action="./" role="search">
			<input id="search-input" type="text" name="s" class="inputbox" value="Search" onfocus="if (value =='Search'){value =''}" onblur="if (value ==''){value='Search'}">
		</form>
		<ul>
      
				
        <li><a class="main-nav-link" href="/">Home</a></li>
      
				
        <li><a class="main-nav-link" href="/archives">Archives</a></li>
      
		</ul>
	</nav>
</header>
    <section id="main" class="clearfix">
  <article class="post-detail">
    <h1 class="post-title"><a href="/">data augmentation</a></h1>
    <ul class="post-meta">
  <li><i class="fa fa-user"></i> Author kopaba</li>
  <li><i class="fa fa-calendar"></i> Date 10月 29</li>
  <li><i class="fa fa-folder"></i> Categories
  
    no_categories
  
  </li>
</ul>
    <div class="post-content">
      <p>(1)将图像从左向右翻转<br>(2)将图像从上到下翻转<br>(3)对角翻转图像<br>(4)调整图像亮度，最大delta设置为0.4<br>(5)调整图像对比度，将比例从0.8调至1.2<br>(6)调整图像色相，将最大差值设置为0.5<br>(7)调整图像饱和度，将比例从0.8设置为1.2<br>(8)将图像旋转90度、180度和270度。</p>

    </div>
  </article>

  <article class="post-detail">
    <h1 class="post-title"><a href="/">batch,epoch,iteration</a></h1>
    <ul class="post-meta">
  <li><i class="fa fa-user"></i> Author kopaba</li>
  <li><i class="fa fa-calendar"></i> Date 10月 29</li>
  <li><i class="fa fa-folder"></i> Categories
  
    no_categories
  
  </li>
</ul>
    <div class="post-content">
      <ol>
<li><p>batch size：简单来说，batch size就是每次训练向模型中传入的数据量的多少，过小的话会导致梯度不明显，模型下降速度慢，过大的话尽管速度会更快但会导致需要更多的epoch来获得更好的结果，与batch本身的节约内存空间的目的相悖，因此需要选择一个合适的batch大小来进行训练。</p>
</li>
<li><p>iteration:意思为迭代，一个iteration等于将batch size中的数据迭代一遍</p>
</li>
<li><p>epoch:意思为周期，一个周期相当于将样本中的所有数据遍历一遍。例如样本数量为1000，batch size为10，那么就需要100此iteration来完成一个epoch。</p>
</li>
</ol>

    </div>
  </article>

  <article class="post-detail">
    <h1 class="post-title"><a href="/">TensorFlow2.x学习笔记</a></h1>
    <ul class="post-meta">
  <li><i class="fa fa-user"></i> Author kopaba</li>
  <li><i class="fa fa-calendar"></i> Date 8月 26</li>
  <li><i class="fa fa-folder"></i> Categories
  
    no_categories
  
  </li>
</ul>
    <div class="post-content">
      <h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>创建变量等操作与1.x基本类似，但可以进行即时执行模式，与之前的图执行模式不同，语法更精炼简单。</p>
<pre class="line-numbers language-python"><code class="language-python">num_epoch <span class="token operator">=</span> <span class="token number">100000</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>learning_rate <span class="token operator">=</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> e <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
        y_pred <span class="token operator">=</span> a <span class="token operator">*</span> x <span class="token operator">+</span> b
        loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>y <span class="token operator">-</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    grads <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss <span class="token punctuation">,</span> variables<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>grads_and_vars <span class="token operator">=</span> zip<span class="token punctuation">(</span>grads <span class="token punctuation">,</span> variables<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以直接调用GradientTape来进行梯度以及导数计算。上面的代码中声明了一个梯度下降优化器，可以通过计算的求导结果更新模型参数，从而最小化某个特定的损失函数，通过apply_gradients()方法来进行调用，函数内的参数为需要更新的变量以及损失函数关于该变量的偏导数。需要传入一个列表，每个元素为(偏导，变量)。</p>
<h2 id="模型与层"><a href="#模型与层" class="headerlink" title="模型与层"></a>模型与层</h2><p>使用TF内置的库Keras来进行模型的构建。<br>Keras里有两个重要的概念：<strong>层(Layer)</strong>和<strong>模型(model)</strong>,层将各种计算流程和变量进行了封装(全连接层，卷积层，池化层等)，模型则用于各种层进行连接。模型通过类的形式呈现，所以可以通过继承tf.keras.Model来定义自己的模型，需要重写<strong>init</strong>()和call(),继承模型后可以调用模型中的方法和属性。可以大量简化代码,下面是通过模型的方式编写的一个简单的线性回归，可以看出在进行计算处理部分与之前基本一致，但是关于模型变量的访问，以及模型初始化都方便了许多。通过在模型内部实例化了一个全连接层，并对其在call()中进行调用</p>
<pre class="line-numbers language-python"><code class="language-python">X <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4.0</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">,</span> <span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">20.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>
            units <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
            activation<span class="token operator">=</span>None<span class="token punctuation">,</span>
            kernel_initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            bias_initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self <span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
model <span class="token operator">=</span> Linear<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
        y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
    grads <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss <span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>grads_and_vars <span class="token operator">=</span> zip<span class="token punctuation">(</span>grads <span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>全连接层(Dense)</strong> 是最常用的层之一，对输入矩阵进行f(AW+b)的线性变换+激活函数操作，激活函数一般是Relu等等.包含的参数如下</p>
<ul>
<li>units 输出张量的维度</li>
<li>activation 激活函数 不指定时为f(x) = x</li>
<li>use_bias 添加偏置，默认为True</li>
<li>kernel_initializer,bias_initializer 权重，偏置的初始化器，默认为glort_uniform_initializer(很多层都默认使用)，使用ezeros_initializer表示初始化为0</li>
</ul>
<p><strong>softmax函数</strong>为了使得模型的输出能始终满足这两个条件，我们使用 Softmax 函数 （归一化指数函数， tf.nn.softmax ）对模型的原始输出进行归一化 。不仅如此，softmax 函数能够凸显原始向量中最大的值，并抑制远低于最大值的其他分量，这也是该函数被称作 softmax 函数的原因（即平滑化的 argmax 函数）。</p>
<h3 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h3><p>tf.keras.losses 和 tf.keras.optimizer<br>需要定义一些模型超参数</p>
<ul>
<li><p>num_epochs = 5</p>
</li>
<li><p>batch_size = 50</p>
</li>
<li><p>learning_rate = 0.001<br>之后实例化模型和优化器，迭代进行数据的读取以及模型的训练步骤如下</p>
</li>
<li><p>从 DataLoader 中随机取一批训练数据；</p>
</li>
<li><p>将这批数据送入模型，计算出模型的预测值；</p>
</li>
<li><p>将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数</p>
</li>
<li><p>计算损失函数关于模型变量的导数；</p>
</li>
<li><p>将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数</p>
</li>
</ul>
<h3 id="模型的评估"><a href="#模型的评估" class="headerlink" title="模型的评估"></a>模型的评估</h3><p>使用tf.keras.metrics中的SparseCategoricalAccuracy来评估模型在测试集上的性能，通过将预测结果与真是结果相比较，输出正确的占比。通过update_state()方法向评估器输入预测值和真实值。其内部有变量来保存当前评估的指标的数值，最终通过result()方法输出最终评估值。</p>
<h3 id="代码汇总"><a href="#代码汇总" class="headerlink" title="代码汇总"></a>代码汇总</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token comment" spellcheck="true">#CNN with keras</span>
<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>
            filters<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>             <span class="token comment" spellcheck="true"># 卷积层神经元（卷积核）数目</span>
            kernel_size<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true"># 感受野大小</span>
            padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true"># padding策略（vaild 或 same）</span>
            activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu   <span class="token comment" spellcheck="true"># 激活函数</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>
            filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>
            kernel_size<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>
            activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Reshape<span class="token punctuation">(</span>target_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>                  <span class="token comment" spellcheck="true"># [batch_size, 28, 28, 32]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                       <span class="token comment" spellcheck="true"># [batch_size, 14, 14, 32]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                       <span class="token comment" spellcheck="true"># [batch_size, 14, 14, 64]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                       <span class="token comment" spellcheck="true"># [batch_size, 7, 7, 64]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                     <span class="token comment" spellcheck="true"># [batch_size, 7 * 7 * 64]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                      <span class="token comment" spellcheck="true"># [batch_size, 1024]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                      <span class="token comment" spellcheck="true"># [batch_size, 10]</span>
        output <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
<span class="token comment" spellcheck="true"># MNISTLoader and MLP</span>
<span class="token keyword">class</span> <span class="token class-name">MNISTLoader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
        <span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_label<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道</span>
        self<span class="token punctuation">.</span>train_data <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_data<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># [60000, 28, 28, 1]</span>
        self<span class="token punctuation">.</span>test_data <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_data<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># [10000, 28, 28, 1]</span>
        self<span class="token punctuation">.</span>train_label <span class="token operator">=</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># [60000]</span>
        self<span class="token punctuation">.</span>test_label <span class="token operator">=</span> self<span class="token punctuation">.</span>test_label<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># [10000]</span>
        self<span class="token punctuation">.</span>num_train_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_test_data <span class="token operator">=</span> self<span class="token punctuation">.</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">get_batch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 从数据集中随机取出batch_size个元素并返回</span>
        index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>train_data<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">[</span>index<span class="token punctuation">]</span>




<span class="token comment" spellcheck="true"># class MLP(tf.keras.Model):</span>
<span class="token comment" spellcheck="true">#     def __init__(self, *args, **kwargs):</span>
<span class="token comment" spellcheck="true">#         super().__init__(*args, **kwargs)</span>
<span class="token comment" spellcheck="true">#         self._flatten = tf.keras.layers.Flatten()</span>
<span class="token comment" spellcheck="true">#         self.dense1 = tf.keras.layers.Dense(units=100 , activation = tf.nn.relu)</span>
<span class="token comment" spellcheck="true">#         self.dense2 = tf.keras.layers.Dense(units=10)</span>
<span class="token comment" spellcheck="true">#     def call(self, inputs):</span>
<span class="token comment" spellcheck="true">#         x = self._flatten(inputs)</span>
<span class="token comment" spellcheck="true">#         x = self.dense1(x)</span>
<span class="token comment" spellcheck="true">#         x = self.dense2(x)</span>
<span class="token comment" spellcheck="true">#         output = tf.nn.softmax(x)</span>
<span class="token comment" spellcheck="true">#         return output</span>
num_epochs <span class="token operator">=</span> <span class="token number">5</span>
batch_size <span class="token operator">=</span> <span class="token number">50</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.001</span>
model <span class="token operator">=</span> CNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
data_loader <span class="token operator">=</span> MNISTLoader<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>learning_rate <span class="token operator">=</span> learning_rate<span class="token punctuation">)</span>
num_batches <span class="token operator">=</span> int<span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>num_train_data <span class="token operator">//</span> batch_size <span class="token operator">*</span> num_epochs<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token punctuation">,</span> y <span class="token operator">=</span> data_loader<span class="token punctuation">.</span>get_batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
        y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>sparse_categorical_crossentropy<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y <span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_pred<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"batch %d: loss %f"</span><span class="token operator">%</span><span class="token punctuation">(</span>batch_index <span class="token punctuation">,</span> loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    grads <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss <span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>grads_and_vars <span class="token operator">=</span> zip<span class="token punctuation">(</span>grads <span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span><span class="token punctuation">)</span>
sparse_categorical_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>SparseCategoricalAccuracy<span class="token punctuation">(</span><span class="token punctuation">)</span>
num_batches <span class="token operator">=</span> int<span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>num_test_data <span class="token operator">//</span> batch_size<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    start_index <span class="token punctuation">,</span> end_index <span class="token operator">=</span> batch_index <span class="token operator">*</span> batch_size <span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> batch_size
    y_pred <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>test_data<span class="token punctuation">[</span>start_index <span class="token punctuation">:</span> end_index<span class="token punctuation">]</span><span class="token punctuation">)</span>
    sparse_categorical_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_true <span class="token operator">=</span> data_loader<span class="token punctuation">.</span>test_label<span class="token punctuation">[</span>start_index <span class="token punctuation">:</span> end_index<span class="token punctuation">]</span> <span class="token punctuation">,</span> y_pred<span class="token operator">=</span> y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"test accuracy : %f"</span><span class="token operator">%</span>sparse_categorical_accuracy<span class="token punctuation">.</span>result<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#using Model for linear regression</span>
<span class="token comment" spellcheck="true"># X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span>
<span class="token comment" spellcheck="true"># y = tf.constant([[10.0], [20.0]])</span>

<span class="token comment" spellcheck="true"># class Linear(tf.keras.Model):</span>
<span class="token comment" spellcheck="true">#     def __init__(self):</span>
<span class="token comment" spellcheck="true">#         super().__init__()</span>
<span class="token comment" spellcheck="true">#         self.dense = tf.keras.layers.Dense(</span>
<span class="token comment" spellcheck="true">#             units = 1,</span>
<span class="token comment" spellcheck="true">#             activation=None,</span>
<span class="token comment" spellcheck="true">#             kernel_initializer = tf.zeros_initializer(),</span>
<span class="token comment" spellcheck="true">#             bias_initializer = tf.zeros_initializer()</span>
<span class="token comment" spellcheck="true">#         )</span>
<span class="token comment" spellcheck="true">#     def call(self , input):</span>
<span class="token comment" spellcheck="true">#         output = self.dense(input)</span>
<span class="token comment" spellcheck="true">#         return output</span>
<span class="token comment" spellcheck="true"># model = Linear()</span>
<span class="token comment" spellcheck="true"># optimizer = tf.keras.optimizers.SGD(lr=0.01)</span>
<span class="token comment" spellcheck="true"># for i in range(100):</span>
<span class="token comment" spellcheck="true">#     with tf.GradientTape() as tape:</span>
<span class="token comment" spellcheck="true">#         y_pred = model(X)</span>
<span class="token comment" spellcheck="true">#         print(model.variables)</span>
<span class="token comment" spellcheck="true">#         loss = tf.reduce_mean(tf.square(y_pred - y))</span>
<span class="token comment" spellcheck="true">#     grads = tape.gradient(loss , model.variables)</span>
<span class="token comment" spellcheck="true">#     optimizer.apply_gradients(grads_and_vars = zip(grads , model.variables))</span>
<span class="token comment" spellcheck="true"># print(model.variables)</span>
<span class="token comment" spellcheck="true"># 1st the linear regression</span>
<span class="token comment" spellcheck="true"># X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)</span>
<span class="token comment" spellcheck="true"># y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)</span>

<span class="token comment" spellcheck="true"># x = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())</span>
<span class="token comment" spellcheck="true"># y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())</span>

<span class="token comment" spellcheck="true"># x = tf.constant(x)</span>
<span class="token comment" spellcheck="true"># y = tf.constant(y)</span>

<span class="token comment" spellcheck="true"># a = tf.Variable(initial_value = 0.)</span>
<span class="token comment" spellcheck="true"># b = tf.Variable(initial_value = 0 , dtype = tf.float32)</span>
<span class="token comment" spellcheck="true"># variables = [a,b]</span>

<span class="token comment" spellcheck="true"># num_epoch = 100000</span>
<span class="token comment" spellcheck="true"># optimizer = tf.keras.optimizers.SGD(learning_rate = 5e-4)</span>
<span class="token comment" spellcheck="true"># for e in range(num_epoch):</span>
<span class="token comment" spellcheck="true">#     with tf.GradientTape() as tape:</span>
<span class="token comment" spellcheck="true">#         y_pred = a * x + b</span>
<span class="token comment" spellcheck="true">#         loss = tf.reduce_sum(tf.square(y - y_pred))</span>
<span class="token comment" spellcheck="true">#     grads = tape.gradient(loss , variables)</span>
<span class="token comment" spellcheck="true">#     optimizer.apply_gradients(grads_and_vars = zip(grads , variables))</span>
<span class="token comment" spellcheck="true"># print(a,b)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

    </div>
  </article>


</section>
    <footer id="footer" role="contentinfo">
    <div>&copy; 2022 <a href="/">Kopaba</a>.
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Designed by <a href="http://rainylog.com" target="_blank">Rainy.</a>
	</div>
</footer><!-- end #footer -->

  </div>
  <script src="/lib/fancybox/dist/jquery.fancybox.min.js"></script>

  <script src="/js/helper.js"></script>
  <script src="/js/_third-party/gitment.js"></script>
</body>
</html>